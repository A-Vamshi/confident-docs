---
subtitle: LLM Tracing Environments
slug: llm-tracing/advanced-features/environment
description: Learn about environments in LLM Tracing
---

The environment feature allows you to specify which environment your traces are coming from. This is useful for separating traces from different environments in `"development"`, `"staging"`, `"production"`, or `"testing"`.

<Callout>
Traces from [component-level evals](/docs/llm-evaluation/component-level-evals) are automatically classified in the `"testing"` enviornment.
</Callout>

## Configure Environment

You can configure the environment with the `CONFIDENT_TRACE_ENVIRONMENT` environment variable. 

<CodeBlock>
```bash
export CONFIDENT_TRACE_ENVIRONMENT="staging"
```
</CodeBlock>

Alternatively, you can set the environment directly in code:

<Tabs>
    <Tab title="Python">
        <CodeBlock>
            ```python title="main.py" {4}
            from openai import OpenAI 
            from deepeval.tracing import observe, trace_manager
            
            trace_manager.configure(environment="production")
            client = OpenAI()
            
            @observe()
            def llm_app(query: str):
                return client.chat.completions.create(
                    model="gpt-4o",
                    messages=[{"role": "user", "content": query}]
                ).choices[0].message.content
            
            llm_app("Write me a poem.")
            ```
        </CodeBlock>
    </Tab>
    <Tab title="Js/TypeScript">
        <CodeBlock>
            ```typescript title="index.ts" {4}
            import OpenAI from 'openai';
            import { observe, traceManager } from '@deepeval-ts/tracing';

            traceManager.configure({ environment: "production" });
            const openai = new OpenAI();

            const llmApp = async (query: string) => {
            const response = await openai.chat.completions.create({
                model: "gpt-4o",
                messages: [{ role: "user", content: query }],
            });
            return response.choices[0].message.content;
            };

            const observedLlmApp = observe({
            type: "llm",
            model: "gpt-4o",
            fn: llmApp,
            });

            observedLlmApp("Write me a poem.");
            ```
        </CodeBlock>
    </Tab>
</Tabs>

The `environment` can be either `"production"`, `"staging"`, or `"development"`, and helps you identify where your traces are coming from.
