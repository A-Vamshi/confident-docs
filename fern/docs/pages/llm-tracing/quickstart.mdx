---
title: LLM Tracing Quickstart
subtitle: Setup LLM observability for your application in less than 5 minutes
slug: llm-tracing/quickstart
---

## Overview

Confident AI allows anyone building with any framework, language, and LLMs to setup LLM observability through tracing.

<Tabs>
  <Tab title="Traces">
    Traces are a single execution of your LLM app, and running evals on traces
    is akin to the [end-to-end
    evals](/docs/llm-evaluation/single-turn/end-to-end) for single-turn
    evaluation in development.
    <Frame caption="LLM Tracing: Traces with Evals">
      <video
        src="https://confident-docs.s3.us-east-1.amazonaws.com/llm-tracing:traces.mp4"
        controls
      />
    </Frame>
  </Tab>
  <Tab title="Spans">
    Spans are individual components such as LLM calls, agents, retrieveres,
    tools, etc. that make up a trace. Running evals on spans is akin to the
    [component-level evals](/docs/llm-evaluation/single-turn/component-level)
    for multi-turn evaluation in development.
    <Frame caption="LLM Tracing: Spans with Evals">
      <video
        src="https://confident-docs.s3.us-east-1.amazonaws.com/llm-tracing:spans.mp4"
        controls
      />
    </Frame>
  </Tab>
  <Tab title="Threads">
    Threads are **a group of traces**, grouped together via a `thread_id` that
    you provide during tracing. It represents a multi-turn LLM interaction,
    exactly the same as [E2E multi-turn
    evals.](/docs/llm-evaluation/multi-turn/end-to-end)
    <Frame caption="LLM Tracing: Threads with Evals">
      <video
        src="https://confident-docs.s3.us-east-1.amazonaws.com/llm-tracing:threads.mp4"
        controls
      />
    </Frame>
  </Tab>
</Tabs>

## Trace Your First LLM App

<Tabs>
  <Tab title="Python">
    <Steps>

      <Step title="Installation">
        Install DeepEval and setup your tracing environment:

          ```bash
          pip install -U deepeval
          ```
      </Step>

      <Step title="Login to Confident AI">
        Get your [Confident AI Project API](https://app.confident-ai.com) key and login.

        <Warning>
          You'll need to get your API key as shown in the [setup and
          installation](/docs/setup-and-installation) section before continuing.
        </Warning>
        <CodeBlocks>

          ```python title="In code"
          from deepeval.tracing import trace_manager

          trace_manager.configure(
              confident_api_key="YOUR-API-KEY"
          )
          ```
          ```bash title="Set Env"
          export CONFIDENT_API_KEY=YOUR-API-KEY
          ```
          ```bash title="Login through CLI"
          deepeval login
          ```

        </CodeBlocks>
      </Step>

      <Step title="Setup Tracing">
        The `@observe` decorator is the primary way to instrument your LLM application for tracing.

          ```python title="main.py" focus={2,6,17}
          from openai import OpenAI
          from deepeval.tracing import observe

          client = OpenAI()

          @observe()
          def llm_app(query: str) -> str:
              return client.chat.completions.create(
                  model="gpt-4o",
                  messages=[
                      {"role": "user", "content": query}
                  ]
              ).choices[0].message.content
              return

          # Call app to send trace to Confident AI
          llm_app("Write me a poem.")
          ```

        If your `llm_app` has more than one function, simply decorate those functions with `@observe` too.

        âœ… You just created a trace with a span inside it. Go to the Observatory to see your traces there.

        <Accordion title="What is a Trace and Span?">
          - **Trace**: The overall process of tracking and visualizing the execution flow of your LLM application
          - **Span**: Individual units of work within your application (e.g., LLM calls, tool executions, retrievals)

          Each observed function **CREATES A SPAN**, and **MANY SPANS MAKE UP A TRACE**. When you have tracing setup, you can run evaluations on both the trace and span level.
        </Accordion>

        In a later section, you'll learn how to create [spans that are LLM specific](/docs/llm-tracing/tracing-features/span-types#llm-span), which allow you to log things like token cost and model name automatically.
      </Step>

      <Step title="Run An Online Evaluation">
        You can run online evaluation on both [end-to-end](/docs/llm-evaluation/single-turn/end-to-end) (metrics on the trace) and [component-level](/docs/llm-evaluation/single-turn/component-level) (metrics on the span) evaluations on Confident AI.

        First [create a metric collection](/docs/llm-evaluation/metrics/create-on-the-cloud#create-a-metric-collection), and add at least one referenceless metric to it. Now in your code, add these lines to automatically run online evals in production:

          ```python title="main.py" focus={2,6,15,19}
          from openai import OpenAI
          from deepeval.tracing import observe, update_current_span

          client = OpenAI()

          @observe(metric_collection=["My Metrics"])
          def llm_app(query: str) -> str:
              res = client.chat.completions.create(
                  model="gpt-4o",
                  messages=[
                      {"role": "user", "content": query}
                  ]
              ).choices[0].message.content

              update_current_span(test_case=LLMTestCase(input=query, actual_output=res))
              return res

          # Call app to send trace to Confident AI
          llm_app("Write me a poem.")
          ```
      </Step>

    </Steps>

  </Tab>

  <Tab title="TypeScript">
    <Steps>

      <Step title="Installation">
        Install DeepEval and setup your tracing environment:
        <CodeBlocks>

          ```bash title="npm"
          npm install deepeval-ts
          ```
          ```bash title="yarn"
          yarn add deepeval-ts
          ```

        </CodeBlocks>
      </Step>

      <Step title="Login to Confident AI">
        Get your [Confident AI Project API](https://app.confident-ai.com) key and login.

        <CodeBlocks>
          ```js title="In code"
          import { traceManager } from '@deepeval-ts/tracing';

          traceManager.configure({
              confidentApiKey: "YOUR-API-KEY"
          })
          ```
          ```bash title="Set Env"
          export CONFIDENT_API_KEY=YOUR-API-KEY
          ```
        </CodeBlocks>
      </Step>

      <Step title="Setup Tracing">
        The `observe` wrapper is the primary way to instrument your LLM application for tracing.

          ```ts title="index.ts" focus={2,13,16}
          import OpenAI from 'openai';
          import { observe } from '@deepeval-ts/tracing';

          const llmApp = async (query: string) => {
            const openai = new OpenAI();
            const res = await openai.chat.completions.create({
              model: "gpt-4o",
              messages: [{ role: "user", content: query }],
            });
            return res.choices[0].message.content;
          };

          const observedLlmApp = observe({ fn: llmApp });

          // Call app to send trace to Confident AI
          observedLlmApp("Write me a poem.");
          ```

        If your `llmApp` has more than one function, simply wrap those functions in `observe` too and call the wrapped functions in `llmApp` instead.

        âœ… You just created a trace with a span inside it. Go to the Observatory to see your traces there.

        <Accordion title="What is a Trace and Span?">
          - **Trace**: The overall process of tracking and visualizing the execution flow of your LLM application
          - **Span**: Individual units of work within your application (e.g., LLM calls, tool executions, retrievals)

          Each observed function **CREATES A SPAN**, and **MANY SPANS MAKE UP A TRACE**. When you have tracing setup, you can run evaluations on both the trace and span level.
        </Accordion>

        In a later section, you'll learn how to create [spans that are LLM specific](/docs/llm-tracing/tracing-features/span-types#llm-span), which allow you to log things like token cost and model name automatically.
      </Step>

      <Step title="Run An Online Evaluation">
        You can run online evaluation on both [end-to-end](/docs/llm-evaluation/single-turn/end-to-end) (metrics on the trace) and [component-level](/docs/llm-evaluation/single-turn/component-level) (metrics on the span) evaluations on Confident AI.

        First [create a metric collection](/docs/llm-evaluation/metrics/create-on-the-cloud#create-a-metric-collection), and add at least one referenceless metric to it. Now in your code, add these lines to automatically run online evals in production:

          ```ts title="index.ts" focus={2,13-15,19-22,25}
          import OpenAI from 'openai';
          import { observe, updateCurrentSpan } from '@deepeval-ts/tracing';

          const openai = new OpenAI();
          const llmApp = async (query: string) => {
            const res = await openai.chat.completions.create({
              model: "gpt-4o",
              messages: [
                { role: "user", content: query }
              ]
            });

            updateCurrentSpan(
              testCase = new LLMTestCase({ input: query, actualOutput: res })
            )
            return res;
          };

          const observedLlmApp = observe({
            metricCollection: ["My Metrics"],
            fn: llmApp
          });

          // Call app to send trace to Confident AI
          observedLlmApp("Write me a poem.");
          ```
      </Step>

    </Steps>

  </Tab>
</Tabs>

**Congratulations!** ðŸŽ‰ Now whenever you run your LLM app, all traces will be logged AND evaluated on Confident AI. Go to the the **Observatory** section on Confident AI to check it out.
