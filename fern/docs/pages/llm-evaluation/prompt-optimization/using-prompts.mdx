---
subtitle: Learn how to test and use prompts in your LLM app
slug: llm-evaluation/prompt-optimization/using-prompts
---

## Overview

You can pull a prompt version from Confident AI like how you would pull a dataset. It works by:

- Providing Confident AI with the alias and optionally version of the prompt you wish to retrieve
- Confident AI will provide the non-interpolated version of the prompt
- You will then interpolate the variables in code

You should pull prompts once and save it in memory instead of pulling it everytime you need to use it.

## Using Prompt Versions

<Steps>

<Step title="Pull prompt with alias">

Pull your prompt version by providing the `alias` you've defined:

<Tabs>

<Tab title="Python" language="python">

```python
from deepeval.prompt import Prompt

prompt = Prompt(alias="YOUR-PROMPT-ALIAS")
prompt.pull()
```

</Tab>

<Tab title="TypeScript" language="typescript">

```typescript
import { Prompt } from "deepeval-ts";

const prompt = new Prompt({ alias: "YOUR-PROMPT-ALIAS" });
await prompt.pull();
```

</Tab>

<Tab title="curL" language="curl">
  <EndpointRequestSnippet endpoint="GET /v1/prompts" />
</Tab>

</Tabs>

<Tip>
  By default, Confident AI will return the latest version of your prompt.
  However, you can also specify the `version` to override this behavior.
</Tip>

</Step>

<Step title="Interpolate variables">

Now that you have your prompt template, interpolate any dynamic variables you may have defined in your prompt version. For example, if this is your prompt version:

<Tabs>

<Tab title="Messages" language="messages">

```json
{
  "role": "system",
  "content": "You are a helpful assistant called {{ name }}. Speak normally like a human."
}
```

And your interpolation type is `{{ variable }}`, interpolating the name (e.g. "Joe") would give you this prompt that is ready for use:

```json
{
  "role": "system",
  "content": "You are a helpful assistant called Joe. Speak normally like a human."
}
```

<Tabs>

<Tab title="Python" language="python">

```python
interpolated_prompt = prompt.interpolate(name="Joe")
```

</Tab>
<Tab title="TypeScript" language="typescript">

```typescript
const interpolatedPrompt = prompt.interpolate({ name: "Joe" });
```

</Tab>

<Tab title="curL" language="curl">

```bash
# Interpolation is done client-side after pulling the prompt
# The API response includes an "interpolationType" field indicating the format:
# - "FSTRING": Use {{ variable }} format (default)
# - "HANDLEBARS": Use {{variable}} format
# Replace variables manually based on the interpolationType in your application code
```

</Tab>

</Tabs>

</Tab>

<Tab title="Text" language="text">

```plaintext
You are a helpful assistant called {{ name }}. Speak normally like a human.
```

And your interpolation type is `{{ variable }}`, interpolating the name (e.g. “Joe”) would give you this prompt that is ready for use:

```plaintext
You are a helpful assistant called Joe. Speak normally like a human.
```

<Tabs>

<Tab title="Python" language="python">

```python
interpolated_prompt = prompt.interpolate(name="Joe")
```

</Tab>
<Tab title="TypeScript" language="typescript">

```typescript
const interpolatedPrompt = prompt.interpolate({ name: "Joe" });
```

</Tab>

<Tab title="curL" language="curl">

```bash
# Interpolation is done client-side after pulling the prompt
# The API response includes an "interpolationType" field indicating the format:
# - "FSTRING": Use {{ variable }} format (default)
# - "HANDLEBARS": Use {{variable}} format
# Replace variables manually based on the interpolationType in your application code
```

</Tab>

</Tabs>

</Tab>
</Tabs>

And if you don't have any variables, you must still use the `interpolate()` method to create a copy of your prompt template to be used in your LLM application.

</Step>

<Step title="Use interpolated prompt">

By now you should have an interpolated prompt version, for example:

<Tabs>

<Tab title="Messages" language="messages">

```json
{
  "role": "system",
  "content": "You are a helpful assistant called Joe. Speak normally like a human."
}
```

Which you can use to generate text from your LLM provider of choice. Here are some examples with OpenAI:

<Tabs>

<Tab title="Python" language="python">

```python main.py {10}
from deepeval.prompt import Prompt
from openai import OpenAI

prompt = Prompt(alias="YOUR-PROMPT-ALIAS")
prompt.pull()
interpolated_prompt = prompt.interpolate() # interpolate prompt

response = OpenAI().chat.completions.create(
    model="gpt-4o-mini",
    messages=interpolated_prompt
)

print(response.choices[0].message.content)
```

</Tab>

<Tab title="TypeScript" language="typescript" >

```typescript index.ts {11}
import { Prompt } from "deepeval-ts";
import { OpenAI } from "openai";

const prompt = new Prompt({ alias: "YOUR-PROMPT-ALIAS" });
await prompt.pull();
const interpolatedPrompt = prompt.interpolate(); // interpolate prompt

const openai = new OpenAI();
const response = await openai.chat.completions.create({
  model: "gpt-3.5-turbo",
  messages: interpolatedPrompt as any[],
});

console.log(response.choices[0].message.content);
```

</Tab>

<Tab title="curL" language="curl">

First, pull the prompt from Confident AI:

<EndpointRequestSnippet endpoint="GET /v1/prompts" />

Then, interpolate the variables and use the interpolated prompt with OpenAI:

```curl
curl -X POST "https://api.openai.com/v1/chat/completions" \
  -H "Authorization: Bearer YOUR_OPENAI_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4.1",
    "messages": <YOUR-INTERPOLATED-PROMPT>
  }'
```

</Tab>

</Tabs>

</Tab>

<Tab title="Text" language="text">

```plaintext
You are a helpful assistant called Joe. Speak normally like a human.
```

Which you can use to generate text from your LLM provider of choice. Here are some examples with OpenAI:

<Tabs>

<Tab title="Python" language="python">

```python main.py {10}
from deepeval.prompt import Prompt
from openai import OpenAI

prompt = Prompt(alias="YOUR-PROMPT-ALIAS")
prompt.pull()
interpolated_prompt = prompt.interpolate() # interpolate prompt

response = OpenAI().chat.completions.create(
    model="gpt-4o-mini",
    messages={"role": "system", "content": interpolated_prompt}
)

print(response.choices[0].message.content)
```

</Tab>

<Tab title="TypeScript" language="typescript">

```typescript index.ts {11}
import { Prompt } from "deepeval-ts";
import { OpenAI } from "openai";

const prompt = new Prompt({ alias: "YOUR-PROMPT-ALIAS" });
await prompt.pull();
const interpolatedPrompt = prompt.interpolate(); // interpolate prompt

const openai = new OpenAI();
const response = await openai.chat.completions.create({
  model: "gpt-4o-mini",
  messages: [{ role: "system", content: interpolatedPrompt }],
});

console.log(response.choices[0].message.content);
```

</Tab>

<Tab title="curL" language="curl">

First, pull the prompt from Confident AI:

<EndpointRequestSnippet endpoint="GET /v1/prompts" />

Then, interpolate the variables and use the interpolated prompt with OpenAI:

```curl
curl -X POST "https://api.openai.com/v1/chat/completions" \
  -H "Authorization: Bearer YOUR_OPENAI_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4.1",
    "messages": <YOUR-INTERPOLATED-PROMPT>
  }'
```

</Tab>

</Tabs>

</Tab>

</Tabs>

</Step>

</Steps>
