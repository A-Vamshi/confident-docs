---
subtitle: Learn how to test and use prompts in your LLM app
slug: llm-evaluation/prompt-optimization/using-prompts
---

## Overview

You can pull a prompt version from Confident AI like how you would pull a dataset. It works by:

- Providing Confident AI with the alias and optionally version of the prompt you wish to retrieve
- Confident AI will provide the non-interpolated version of the prompt
- You will then interpolate the variables in code

You should pull prompts once and save it in memory instead of pulling it everytime you need to use it.

## Using Prompt Versions

<Steps>

<Step title="Pull prompt with alias">

Pull your prompt version by providing the `alias` you've defined:

<Tabs>

  <Tab title="Python" language="python">
  ```python
  from deepeval.prompt import Prompt

prompt = Prompt(alias="YOUR-PROMPT-ALIAS")
prompt.pull()

````
</Tab>

  <Tab title="TypeScript" language="typescript">
  ```typescript
  import { Prompt } from "deepeval-ts";
  // Replace with your actual alias
  const prompt = new Prompt("System Prompt");
  await prompt.pull();

  console.log(prompt.textTemplate);
  ```
  </Tab>

<Tab title="curL" language="curl">
  <EndpointRequestSnippet
    endpoint="GET /v1/prompts"
  />
</Tab>

</Tabs>

<Tip>
By default, Confident AI will return the latest version of your prompt. However, you can also specify the `version` to override this behavior.
</Tip>

</Step>

<Step title="Interpolate variables">


Now that you have your prompt template, interpolate any dynamic variables you may have defined in your prompt version. For example, if this is your prompt version:

<Tabs>

<Tab title="Messages" langauage="messages">

```json
{
  "role": "system",
  "content": "You are a helpful assistant called {{ name }}. Speak normally like a human."
}
```





And your interpolation type is `{{ variable }}`, interpolating the name (e.g. "Joe") would give you this prompt that is ready for use:

```json
{
  "role": "system",
  "content": "You are a helpful assistant called Joe. Speak normally like a human."
}
```

</Tab>

  <Tab title="TypeScript" language="typescript">
  ```typescript
  import { Prompt } from "deepeval-ts";
  // Replace with your actual alias
  const prompt = new Prompt("System Prompt");

  // Replace with your actual version
  await prompt.pull("00.00.01");

  console.log(prompt.textTemplate);
  ```
  </Tab>

  <Tab title="curL" language="curl">
    <EndpointRequestSnippet
      endpoint="GET /v1/prompts"
    />
  </Tab>

</Tabs>



<Tabs>

  <Tab title="Python" language="python">
  ```python
interpolated_prompt = prompt.interpolate(name="Joe")
````

</Tab>

  <Tab title="TypeScript" language="typescript">
  ```typescript
  ...
  const prompt_to_llm = prompt.interpolate({ name: "Joe" });
  console.log(prompt_to_llm);
  ```
  </Tab>

</Tabs>

And if you don't have any variables, you must still use the `interpolate()` method to create a copy of your prompt template to be used in your LLM application.

</Step>

<Step title="Use interpolated prompt">

By now you should have an interpolated prompt version, for example:

<Tabs>

<Tab title="Messages" language="messages">
```json
{
  "role": "system",
  "content": "You are a helpful assistant called Joe. Speak normally like a human."
}
```

Which you can use to generate text from your LLM provider of choice. Here are some examples with OpenAI:

<Tabs>

<Tab title="Python" language="python">

```python main.py {10}
from deepeval.prompt import Prompt
from openai import OpenAI

prompt = Prompt(alias="YOUR-PROMPT-ALIAS")
prompt.pull()
interpolated_prompt = prompt.interpolate() # interpolate prompt

response = OpenAI().chat.completions.create(
    model="gpt-4o-mini",
    messages=interpolated_prompt
)

print(response.choices[0].message.content)
```

</Tab>

  <Tab title="TypeScript" language="typescript">
  ```typescript
  import { Prompt } from "deepeval-ts";
  const prompt = new Prompt("System Prompt");
  await prompt.pull();

const messages_to_llm = prompt.interpolate({});
console.log(messages_to_llm);

// This will print the following
// [
// {"role": "system", "content": "You are a helpful assistant."},
// {"role": "user", "content": "What is the capital of France?"}
// ]

````
</Tab>

<Tab title="curL" language="curl">
  <EndpointRequestSnippet
    endpoint="GET /v1/prompts"
  />
</Tab>

</Tabs>

</Tab>

<Tab title="Text" language="text">

```plaintext
"You are a helpful assistant called Joe. Speak normally like a human."
````

Which you can use to generate text from your LLM provider of choice. Here are some examples with OpenAI:

<Tabs>

<Tab title="Python" language="python">

```python main.py {10}
from deepeval.prompt import Prompt
from openai import OpenAI

prompt = Prompt(alias="YOUR-PROMPT-ALIAS")
prompt.pull()
interpolated_prompt = prompt.interpolate() # interpolate prompt

response = OpenAI().chat.completions.create(
    model="gpt-4o-mini",
    messages={"role": "system", "content": interpolated_prompt}
)

print(response.choices[0].message.content)
```

</Tab>

  <Tab title="TypeScript" language="typescript">
  ```typescript
  import { OpenAI } from "openai";
  import { Prompt } from "deepeval-ts";

async function main() {
const client = new OpenAI();

      // Make sure to replace "your-alias-here" with your actual prompt alias
      const prompt = new Prompt("your-alias-here");
      await prompt.pull();

      // The interpolate method in TypeScript expects an object for replacements,
      // so we pass an empty object if there are no variables to interpolate.
      const messagesToLlm = prompt.interpolate({}) as any[];

      async function yourLlmApp(userInput: string) {
          // Combine the pulled prompt messages with the user's input
          const messages = [...messagesToLlm, { role: "user", content: userInput }];

          const response = await client.chat.completions.create({
              model: "gpt-4",
              messages: messages as any, // Casting to any to match OpenAI's expected type
          });

          return response.choices[0].message.content;
      }

      // Get the response from your LLM application
      const response = await yourLlmApp("Hi!");

      // Print the response
      console.log(response);

}

main().catch(console.error);

```
</Tab>

</Tabs>

</Tab>

</Tabs>

</Step>

</Steps>
```
