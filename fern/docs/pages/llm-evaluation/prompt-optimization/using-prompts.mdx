---
subtitle: Learn how to test and use prompts in your LLM app
slug: llm-evaluation/prompt-optimization/using-prompts
---

## Overview

You can pull a prompt version from Confident AI like how you would pull a dataset. You should pull prompts once and save it in memory instead of pulling it everytime you need to use it.

## Pull a Prompt Version

Pull your prompt version from Confident AI by providing the `alias` you've defined:

<Tabs>

  <Tab title="Python" language="python">
  ```python
  from deepeval.prompt import Prompt

  # Replace with your actual alias

  prompt = Prompt(alias="System Prompt")
  prompt.pull()

  print(prompt._text_template)
  ```
  </Tab>

  <Tab title="TypeScript" language="typescript">
  [TODO kritin]
  </Tab>

  <Tab title="curL" language="curl">
    <EndpointRequestSnippet
      endpoint="GET /v1/prompts"
    />
  </Tab>

</Tabs>

You'll notice that unlike the `pull()` method on an `EvaluationDataset`, the `alias` is actually defined on `Prompt` instantiation instead. This is because the `pull()` method on a `Prompt` is reserved to specify the version instead.

### Specify the Version

When no parameters are provided to the `pull()` method, the latest version is pulled by default. To specify the version, simply supply the `version` as an argument:

<Tabs>

  <Tab title="Python" language="python">
  ```python
  ...

  prompt.pull(version="00.00.01")
  print(prompt._text_template)

  ```
  </Tab>

  <Tab title="TypeScript" language="typescript">
  [TODO kritin]
  </Tab>

  <Tab title="curL" language="curl">
    <EndpointRequestSnippet
      endpoint="GET /v1/prompts"
    />
  </Tab>

</Tabs>

## Interpolate Variables

Now that you have your prompt template, use the `interpolate()` method to interpolate any dynamic variables you may have defined in your prompt version. For example, if you have a `{{ name }}` variable, it will look like this:

<Tabs>

  <Tab title="Python" language="python">
  ```python
  ...

  # Customized accordingly based on your variables
  prompt_to_llm = prompt.interpolate(name="Joe")
  ```
  </Tab>

  <Tab title="TypeScript" language="typescript">
  [TODO kritin]
  </Tab>

</Tabs>

And if you don't have any variables, you should still use the `interpolate()` method to create a copy of your prompt template to be used in your LLM application.

<Error title="Warning">
  Referencing the `template` variable in a `Prompt` class is extremely bad
  practice and should be **avoided at all cost**. Always use the `interpolate()`
  method before supplying prompt templates to your LLM application.
</Error>

## Pull a List of Prompt Messages

You pull prompt messages the same way you would pull a single prompt, the difference is it is now stored in the `_messages_template` variable instead, and you can print it out to see it for yourself:

<Tabs>

  <Tab title="Python" language="python">
  ```python focus={1,3-5,13-14}
  from deepeval.prompt import Prompt

  prompt = Prompt(alias="your-alias-here")
  prompt.pull()
  print(prompt._messages_template)

  # This will print the following
  # [
  #   PromptMessage(role="system", content="You are a helpful assistant."),
  #   PromptMessage(role="user", content="What is the capital of France?"),
  # ]

  messages_to_llm = prompt.interpolate()
  print(messages_to_llm)

  # This will print the following
  # [
  #   {"role": "system", "content": "You are a helpful assistant."},
  #   {"role": "user", "content": "What is the capital of France?"}
  # ]
  ```
  </Tab>

  <Tab title="TypeScript" language="typescript">
  [TODO kritin]
  </Tab>

  <Tab title="curL" language="curl">
    <EndpointRequestSnippet
      endpoint="GET /v1/prompts"
    />
  </Tab>

</Tabs>

To be clear, here is an example of how you can use it for OpenAI's APIs:

<Tabs>

  <Tab title="Python" language="python">
  ```python {5,10}
  from openai import OpenAI
  from deepeval.prompt import Prompt

  client = OpenAI()
  prompt = Prompt(alias="your-alias-here")
  prompt.pull()

  # Don't forget to interpolate it

  messages_to_llm = prompt.interpolate()

  def your_llm_app(user_input: str): # Combine dynamically with user input
  messages = messages_to_llm + [{"role": "user", "content": user_input}]

      response = client.chat.completions.create(
          model="gpt-4",
          messages=messages
      )
      return response.choices[0].message.content

  # Print the response

  print(your_llm_app("Hi!"))
  ```
  </Tab>

  <Tab title="TypeScript" language="typescript">
  [TODO kritin]
  </Tab>

</Tabs>



```

```

```

```
