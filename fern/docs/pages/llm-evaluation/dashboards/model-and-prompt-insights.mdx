---
title: Model and Prompt Insights
subtitle: Learn how to find the best models, prompts, etc. for your LLM app
slug: llm-evaluation/dashboards/model-and-prompt-insights
---

## Overview

When running evaluations, it's important to log hyperparameters to track which configurations were used for each test run. This allows you to compare different versions of prompts, models, and other settings to make data-driven decisions about your LLM application.

<Warning>
  This is currently only supported for [end-to-end
  evaluation](/docs/llm-evaluation/end-to-end-evals).
</Warning>

If you're unsure what are hyperparameters or the types of hyperparameters available, [click here.](/docs/concepts/hyperparameters)

<Frame caption="Prompt and Model Insights">
  <video
    src="https://confident-docs.s3.us-east-1.amazonaws.com/evaluation:parameter-insights.mp4"
    autoPlay
    controls
  />
</Frame>

## Prompts

Prompts are a **type** of hyperparameter, and in Confident AI, you can include them as hyperparameters to [test each prompt's performance](/docs/prompt-management/prompt-testing) when running an evaluation:

<Tabs>

<Tab title="Python" langauag="python">

```python main.py
from deepeval.prompt import Prompt
from deepeval.metrics import AnswerRelevancyMetric
from deepeval.test_case import LLMTestCase
from deepeval import evaluate

# Pull your prompt version
prompt = Prompt(alias="your-prompt-alias")
prompt.pull()

# Run an evaluation with the prompt as a hyperparameter
evaluate(
    test_cases=[LLMTestCase(input="...", actual_output="...")],
    metrics=[AnswerRelevancyMetric()],
    hyperparameters={"System Prompt": prompt}
)
```

</Tab>

<Tab title="Typescript" langauag="typescript">

```ts
evaluate({
  hyperparameters: {
    Model: "YOUR-MODEL",
    "Prompt Version": "YOUR-PROMPT-VERSION",
  },
  llmTestCases: [...],
  metricCollection: "YOUR-COLLECTION-NAME",
});
```

</Tab>

<Tab title="curL" langauag="curl">

<EndpointRequestSnippet
  endpoint="POST /v1/evaluate"
  example="Logging Parameters"
/>

</Tab>

</Tabs>

You should **NEVER** log the interpolated version of your prompt template.
This will create a unique version for each variable substitution, making it
impossible to meaningfully compare prompt performance. Always log the `Prompt`
instance itself.

## Models and Others

You can also log model and other parameter information as hyperparameters to track which model versions or configurations were used for a test run:

<Tabs>

<Tab title="Python" langauag="python">

```python main.py
evaluate(
    test_cases=dataset.test_cases,
    metrics=[AnswerRelevancyMetric()],
    hyperparameters={
        "Model": "gpt-4",
    }
)
```

</Tab>

<Tab title="Typescript" langauag="typescript">

```typescript
evaluate({
  hyperparameters: {
    Model: "YOUR-MODEL",
  },
  llmTestCases: [...],
  metricCollection: "YOUR-COLLECTION-NAME",
});
```

</Tab>

<Tab title="curL" langauag="curl">

[TODO vamshi]

</Tab>

</Tabs>

## In CI/CD

For python users of `deepeval`, you can log parameteres in CI/CD using the `@deepeval.log_hyperparameters()` decorator on top of a function that returns a dictionary of string and prompts:

```python test_llm_app.py
from deepeval.prompt import Prompt
import deepeval

prompt = Prompt(alias="your-prompt-alias")
prompt.pull()

@deepeval.log_hyperparameters()
def hyperparameters():
    return {"System Prompt": prompt, "Model": "YOUR-MODEL-NAME"}
```

Now everytime when you run `deepeval test run`, every test file will be ran as unit-tests while logging parameters as part of your test run. Go to the [next section](/docs/llm-evaluation/unit-testing-cicd) to learn more about unit-testing in CI/CD.
