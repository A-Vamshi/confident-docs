---
subtitle: Answer relevancy is a single-turn metric to evaluate RAG generators
slug: metrics/single-turn/answer-relevancy-metric
---

{/* [pills to show it is available locally on deepeval and on platform] */}

## Overview

The answer relevancy metric uses LLM-as-a-judge to assess whether your RAG generator's output is relevant to the given input. It is a single-turn metric designed specifically for evaluating RAG QA specifically, and not general RAG.

<Warning>

The input of a test case should not contain the entire prompt, but just the query when using the answer relevancy metric.

</Warning>

### Required Parameters

These are the parameters you must supply in your test case to run evaluations for answer relevancy metric:

<ParamField path="input" type="string" required>
  The input query you supply to your RAG application.
</ParamField>
<ParamField path="actual_output" type="string" required>
  The final output your RAG application's generator generates.
</ParamField>

## How Is It Calculated?

The answer relevancy metric first breaks down the actual output of a test case into distinct statements, then calculates the proportion of those statements that are relevant to the given input.

<br />

$$
\text{Answer Relevanacy} = \frac{\text{Number of Relevant Statements}}{\text{Total Number of Statements}}
$$

<br />

The final score is the proportion of relevant statements found in the actual output.

## Create Locally

You can create the `AnswerRelevancyMetric` in `deepeval` as follows:

```python
from deepeval.metrics import AnswerRelevancyMetric

metric = AnswerRelevancyMetric()
```

### Optional Parameters

<ParamField path="threshold" type="number" default={0.5}>
  A float to represent the minimum passing threshold.
</ParamField>
<ParamField path="model" type="string | Object" default="gpt-4.1">
  A string specifying which of OpenAI's GPT models to use OR any custom LLM
  model of type
  [`DeepEvalBaseLLM`](https://deepeval.com/guides/guides-using-custom-llms).
</ParamField>
<ParamField path="include_reason" type="boolean" default={"true"}>
  A boolean to enable the inclusion a reason for its evaluation score.
</ParamField>
<ParamField path="async_mode" type="boolean" default={"true"}>
  A boolean to enable concurrent execution within the `measure()` method.
</ParamField>
<ParamField path="strict_mode" type="boolean" default={"false"}>
  A boolean to enforce a binary metric score: 0 for perfection, 1 otherwise.
</ParamField>
<ParamField path="verbose_mode" type="boolean" default={"false"}>
  A boolean to print the intermediate steps used to calculate the metric score.
</ParamField>
<ParamField
  path="evaluation_template"
  type="AnswerRelevancyTemplate"
  default={"deepeval's template"}
>
  An instance of `AnswerRelevancyTemplate` object, which allows you to override
  the default prompts used to compute the `AnswerRelevancyMetric` score.
</ParamField>

<Success>
  This can be used for both [single-turn
  E2E](/docs/llm-evaluation/single-turn/end-to-end) and
  [component-level](/docs/llm-evaluation/single-turn/component-level) testing.
</Success>

## Create Remotely

For users not using `deepeval` python, or want to run evals remotely on Confident AI, you can use the answer relevancy metric by adding it to a single-turn [metric collection.](/metrics/metric-collections) This will allow you to use answer relevancy metric for:

- Single-turn E2E testing
- Single-turn component-level testing
- Online and offline evals for traces and spans
