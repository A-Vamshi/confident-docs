---
subtitle: Summarization is a single-turn metric to determine if your summarizer is generating facutally correct summaries.
slug: metrics/single-turn/summarization-metric
---

[pills to show it is available locally on deepeval and on platform]

## Overview

The summarization metric uses LLM-as-a-judge to determine whether your LLM (application) is generating factually correct summaries while including the necessary details from the original text. It is a single-turn metric that can be used to evaluate any LLM summarizer for factually accurate summaries.

<Warning>

The summarization metric assumes the original text to be the input and the summary generated as the actual output.

</Warning>

## How Is It Calculated?

The summarization metric breaks the score into `alignment_score` and `coverage_score`.

<br />

$$
\text{Summarization} = \text{min(\text{Alignement Score}, \text{Coverage Score})}
$$

<br />

The final score is the minumum of:

- `alignment_score` which determines whether the summary contains hallucinated or contradictory information to the original text.
- `coverage_score` which determines whether the summary contains the necessary information from the original text.

Here's a good read on [how our summarization metric was developed](https://www.confident-ai.com/blog/a-step-by-step-guide-to-evaluating-an-llm-text-summarization-task).

## Create Locally

You can create the `SummarizationMetric` in `deepeval` as follows:

```python
from deepeval.metrics import SummarizationMetric

metric = SummarizationMetric()
```

### Optional Parameters

<ParamField path="threshold" type="number" default={0.5}>
  A float to represent the minimum passing threshold.
</ParamField>
<ParamField
  path="assessment_questions"
  type="list of strings"
  default={"questions generated by deepeval at evaluation time"}
>
  A list of close-ended questions that can be answered with either a `yes` or a
  `no`. These are questions you want your summary to be able to ideally answer,
  they are helpful for using a custom criteria for a good summary.
</ParamField>
<ParamField path="n" type="number" default="5">
  The number of assessment questions to generate when `assessment_questions` is
  not provided.
</ParamField>
<ParamField path="truths_extraction_limit" type="number">
  An integer which when set, determines the maximum number of factual truths to
  extract from the input.
</ParamField>
<ParamField path="model" type="string | Object" default="gpt-4.1">
  A string specifying which of OpenAI's GPT models to use OR any custom LLM
  model of type
  [`DeepEvalBaseLLM`](https://deepeval.com/guides/guides-using-custom-llms).
</ParamField>
<ParamField path="include_reason" type="boolean" default={"true"}>
  A boolean to enable the inclusion a reason for its evaluation score.
</ParamField>
<ParamField path="async_mode" type="boolean" default={"true"}>
  A boolean to enable concurrent execution within the `measure()` method.
</ParamField>
<ParamField path="strict_mode" type="boolean" default={"false"}>
  A boolean to enforce a binary metric score: 0 for perfection, 1 otherwise.
</ParamField>
<ParamField path="verbose_mode" type="boolean" default={"false"}>
  A boolean to print the intermediate steps used to calculate the metric score.
</ParamField>

<Success>
  This can be used for both [single-turn
  E2E](/docs/llm-evaluation/single-turn/end-to-end) and
  [component-level](/docs/llm-evaluation/single-turn/component-level) testing.
</Success>

## Create Remotely

For users not using `deepeval` python, or want to run evals remotely on Confident AI, you can use the summarization metric by adding it to a single-turn [metric collection.](/metrics/metric-collections) This will allow you to use summarization metric for:

- Single-turn E2E testing
- Single-turn component-level testing
- Online and offline evals for traces and spans
