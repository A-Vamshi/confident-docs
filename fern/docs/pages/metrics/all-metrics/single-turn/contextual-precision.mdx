---
subtitle: Contextual Precision is a single-turn metric used to evaluate a RAG retriever
slug: metrics/single-turn/contextual-precision-metric
---

{/* [pills to show it is available locally on deepeval and on platform] */}

## Overview

The contextual precision metric is a single-turn RAG metric that uses LLM-as-a-judge to evaluate how well your retriever ranks the retrieved context based on the input query.

<Warning>

The input of a test case should not contain the entire prompt, but just the query when using the contextual precision metric.

</Warning>

### Required Parameters

These are the parameters you must supply in your test case to run evaluations for contextual precision metric:

<ParamField path="input" type="string" required>
  The input query you supply to your RAG application.
</ParamField>
<ParamField path="expected_output" type="string" required>
  The expected output your RAG application has to generate for a given input.
</ParamField>
<ParamField path="retrieval_context" type="list of string" required>
  The retrieved context your retriever outputs for a given input sorted by their rank.
</ParamField>

## How Is It Calculated?

The contextual precision metric evaluates each retrieved node using an LLM to check if it is correctly ranked for relevance to the input. It then calculates the final score using the following equation:

<br />

$$
\text{Contextual Precision} = \frac{1}{\text{Num of Relevant Nodes}}\sum_{k=1}^{n}(\frac{\text{Num of Relevant Nodes Upto position k}}{k} \times r_k)
$$

<br />

<Info>
k - `i+1`th node in the retrieval context

n - number of nodes in the retrieval context

râ‚– - the binary relevance of the `k`th node. 1 if relevant, 0 otherwise.
</Info>

A high contextual precison score indicates that all the retrieved nodes are in the order of their relevance to the input.

## Create Locally

You can create the `ContextualPrecisionMetric` in `deepeval` as follows:

```python
from deepeval.metrics import ContextualPrecisionMetric

metric = ContextualPrecisionMetric()
```

### Optional Parameters

<ParamField path="threshold" type="number" default={0.5}>
  A float to represent the minimum passing threshold.
</ParamField>
<ParamField path="model" type="string | Object" default="gpt-4.1">
  A string specifying which of OpenAI's GPT models to use OR any custom LLM
  model of type
  [`DeepEvalBaseLLM`](https://deepeval.com/guides/guides-using-custom-llms).
</ParamField>
<ParamField path="include_reason" type="boolean" default={"true"}>
  A boolean to enable the inclusion a reason for its evaluation score.
</ParamField>
<ParamField path="async_mode" type="boolean" default={"true"}>
  A boolean to enable concurrent execution within the `measure()` method.
</ParamField>
<ParamField path="strict_mode" type="boolean" default={"false"}>
  A boolean to enforce a binary metric score: 0 for perfection, 1 otherwise.
</ParamField>
<ParamField path="verbose_mode" type="boolean" default={"false"}>
  A boolean to print the intermediate steps used to calculate the metric score.
</ParamField>
<ParamField
  path="evaluation_template"
  type="ContextualPrecisionTemplate"
  default={"deepeval's template"}
>
  An instance of `ContextualPrecisionTemplate` object, which allows you to
  override the default prompts used to compute the `ContextualPrecisionMetric`
  score.
</ParamField>

<Success>
  This can be used for both [single-turn
  E2E](/docs/llm-evaluation/single-turn/end-to-end) and
  [component-level](/docs/llm-evaluation/single-turn/component-level) testing.
</Success>

## Create Remotely

For users not using `deepeval` python, or want to run evals remotely on Confident AI, you can use the contextual precision metric by adding it to a single-turn [metric collection.](/metrics/metric-collections) This will allow you to use contextual precision metric for:

- Single-turn E2E testing
- Single-turn component-level testing
- Online and offline evals for traces and spans
