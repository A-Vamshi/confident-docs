---
subtitle: Faithfulness is a single-turn metric to evaluate RAG generators
slug: metrics/single-turn/faithfulness-metric
---

[pills to show it is available locally on deepeval and on platform]

## Overview

The faithfulness metric uses LLM-as-a-judge to measure the quality of your RAG's generator by evaluating whether the actual output factually aligns with the contents of your retrieval context. It is a single-turn metric to evaluate RAG QA specifically, and not general RAG.

## How Is It Calculated?

The faithfulness metric first uses an LLM to extract all claims made in the actual output, and then uses the same LLM to classify whether each claim is truthful based on the facts presented in the retrieval context.

$$
\text{Faithfulness} = \frac{\text{Number of Truthful Claims}}{\text{Total Number of Claims}}
$$

<Tip>
A claim is considered truthful if it does not contradict any facts presented in the retrieval context.
</Tip>

The final score is the proportion of truthful claims found in the actual output.

## Create Locally

You can create the `FaithfulnessMetric` in `deepeval` as follows:

```python
from deepeval.metrics import FaithfulnessMetric

metric = FaithfulnessMetric()
```

### Optional Parameters

<ParamField path="threshold" type="number" default={0.5}>
    A float to represent the minimum passing threshold.
</ParamField>
<ParamField path="model" type="string | Object" default="gpt-4.1">
    A string specifying which of OpenAI's GPT models to use OR any custom LLM model of type [`DeepEvalBaseLLM`](https://deepeval.com/guides/guides-using-custom-llms). 
</ParamField>
<ParamField path="include_reason" type="boolean" default={"true"}>
    A boolean to enable the inclusion a reason for its evaluation score.
</ParamField>
<ParamField path="async_mode" type="boolean" default={"true"}>
    A boolean to enable concurrent execution within the `measure()` method.
</ParamField>
<ParamField path="strict_mode" type="boolean" default={"false"}>
    A boolean to enforce a binary metric score: 0 for perfection, 1 otherwise.
</ParamField>
<ParamField path="verbose_mode" type="boolean" default={"false"}>
    A boolean to print the intermediate steps used to calculate the metric score.
</ParamField>
<ParamField path="evaluation_template" type="FaithfulnessTemplate" default={"deepeval's template"}>
    An instance of `FaithfulnessTemplate` object, which allows you to override the default prompts used to compute the `FaithfulnessMetric` score.
</ParamField>

<Success>
  This can be used for both [single-turn
  E2E](/docs/llm-evaluation/single-turn/end-to-end) and
  [component-level](/docs/llm-evaluation/single-turn/component-level) testing.
</Success>

## Create Remotely

For users not using `deepeval` python, or want to run evals remotely on Confident AI, you can use the faithfulness metric by adding it to a single-turn [metric collection.](/metrics/metric-collections) This will allow you to use faithfulness metric for:

- Single-turn E2E testing
- Single-turn component-level testing
- Online and offline evals for traces and spans
