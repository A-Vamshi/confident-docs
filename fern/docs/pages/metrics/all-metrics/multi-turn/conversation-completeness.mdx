---
subtitle: Conversation Completeness is a multi-turn metric to determine if a conversation is complete.
slug: metrics/multi-turn/conversation-completeness-metric
---

[pills to show it is available locally on deepeval and on platform]

## Overview

The conversational completeness metric is a multi-turn metric that uses LLM-as-a-judge to evaluate whether your chatbot satisfies the userâ€™s requirements at each turn throughout the conversation.

### Required Parameters

These are the parameters you must supply in your test case to run evaluations for conversation completeness metric:

<ParamField path="turns" type="list of Turn" required>
  A list of `Turn`s as exchanges between user and assistant.
  <Card title="Parameters of `Turn`:">
    ---
    <ParamField path="role" type="user | assistant" required>
      The role of the person speaking, it's either `user` or `assistant`
    </ParamField>
    <ParamField path="content" type="string" required>
      The content provided by the `role` for the turn
    </ParamField>
  </Card>
</ParamField>

## How Is It Calculated?

The conversation completeness metric first extracts distinct user intentions from all turns using an LLM, then uses the same LLM to check if the corresponding assistant turns have satisfied those intentions.

<br />

$$
\text{Conversation Completeness} = \frac{\text{Number of Satisfied User Intentions in Conversation}}{\text{Total Number of User Intentions in Conversation}}
$$

<br />

The final score is the proportion of satisfied user intentions found in the conversation.

## Create Locally

You can create the `ConversationCompletenessMetric` in `deepeval` as follows:

```python
from deepeval.metrics import ConversationCompletenessMetric

metric = ConversationCompletenessMetric()
```

### Optional Parameters

<ParamField path="threshold" type="number" default={0.5}>
  A float to represent the minimum passing threshold.
</ParamField>
<ParamField path="model" type="string | Object" default="gpt-4.1">
  A string specifying which of OpenAI's GPT models to use OR any custom LLM
  model of type
  [`DeepEvalBaseLLM`](https://deepeval.com/guides/guides-using-custom-llms).
</ParamField>
<ParamField path="include_reason" type="boolean" default={"true"}>
  A boolean to enable the inclusion a reason for its evaluation score.
</ParamField>
<ParamField path="async_mode" type="boolean" default={"true"}>
  A boolean to enable concurrent execution within the `measure()` method.
</ParamField>
<ParamField path="strict_mode" type="boolean" default={"false"}>
  A boolean to enforce a binary metric score: 0 for perfection, 1 otherwise.
</ParamField>
<ParamField path="verbose_mode" type="boolean" default={"false"}>
  A boolean to print the intermediate steps used to calculate the metric score.
</ParamField>

<Success>
  This can be used for [multi-turn
  E2E](/docs/llm-evaluation/single-turn/end-to-end)
</Success>

## Create Remotely

For users not using `deepeval` python, or want to run evals remotely on Confident AI, you can use the conversation completeness metric by adding it to a single-turn [metric collection.](/metrics/metric-collections) This will allow you to use conversation completeness metric for:

- Multi-turn E2E testing
- Online and offline evals for traces and spans
