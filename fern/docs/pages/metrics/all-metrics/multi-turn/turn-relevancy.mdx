---
subtitle: Turn Relevancy is a multi-turn metric to determine if your chatbot responses are releveant to user input
slug: metrics/multi-turn/turn-relevancy-metric
---

{/* [pills to show it is available locally on deepeval and on platform] */}

## Overview

The turn relevancy metric is a multi-turn metric that uses LLM-as-a-judge to evaluate whether your chatbotâ€™s responses are relevant to the corresponding user inputs at each turn in the conversation.

### Required Parameters

These are the parameters you must supply in your test case to run evaluations for turn relevancy metric:

<ParamField path="turns" type="list of Turn" required>
  A list of `Turn`s as exchanges between user and assistant.
  <Card title="Parameters of `Turn`:">
    ---
    <ParamField path="role" type="user | assistant" required>
      The role of the person speaking, it's either `user` or `assistant`
    </ParamField>
    <ParamField path="content" type="string" required>
      The content provided by the `role` for the turn
    </ParamField>
  </Card>
</ParamField>

## How Is It Calculated?

The turn relevancy metric loops over all the turns to find the `assistant` turns and uses an LLM to see if the corresponding turn's `content` is relevant to the previous user turn's `content`.

<br />

$$
\text{Turn Relevancy} = \frac{\text{Number of Assistant Turns with Relevant Assistant Content}}{\text{Total Number of Assistant Turns}}
$$

<br />

The final score is the proportion of assisant turns that give relevant output in the conversation.

## Create Locally

You can create the `TurnRelevancyMetric` in `deepeval` as follows:

```python
from deepeval.metrics import TurnRelevancyMetric

metric = TurnRelevancyMetric()
```

### Optional Parameters

<ParamField path="threshold" type="number" default={0.5}>
  A float to represent the minimum passing threshold.
</ParamField>
<ParamField path="window_size" type="number" default={10}>
  An integer which defines the size of the sliding window of turns used during
  evaluation.
</ParamField>
<ParamField path="model" type="string | Object" default="gpt-4.1">
  A string specifying which of OpenAI's GPT models to use OR any custom LLM
  model of type
  [`DeepEvalBaseLLM`](https://deepeval.com/guides/guides-using-custom-llms).
</ParamField>
<ParamField path="include_reason" type="boolean" default={"true"}>
  A boolean to enable the inclusion a reason for its evaluation score.
</ParamField>
<ParamField path="async_mode" type="boolean" default={"true"}>
  A boolean to enable concurrent execution within the `measure()` method.
</ParamField>
<ParamField path="strict_mode" type="boolean" default={"false"}>
  A boolean to enforce a binary metric score: 0 for perfection, 1 otherwise.
</ParamField>
<ParamField path="verbose_mode" type="boolean" default={"false"}>
  A boolean to print the intermediate steps used to calculate the metric score.
</ParamField>

<Success>
  This can be used for [multi-turn
  E2E](/docs/llm-evaluation/single-turn/end-to-end)
</Success>

## Create Remotely

For users not using `deepeval` python, or want to run evals remotely on Confident AI, you can use the turn relevancy metric by adding it to a single-turn [metric collection.](/metrics/metric-collections) This will allow you to use turn relevancy metric for:

- Multi-turn E2E testing
- Online and offline evals for traces and spans
