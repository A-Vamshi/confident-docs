---
subtitle: Proactively identify vulnerabilities and safety issues in your AI applications through adversarial testing.
slug: red-teaming/introduction
description: Get started with Confident AI's Red Teaming for AI safety and security assessment
---

## What is Confident AI red teaming?

Red Teaming on Confident AI is a comprehensive adversarial testing platform that helps organizations proactively identify vulnerabilities, safety issues, and harmful behaviors in their AI applications before they reach production.

**AI Red Teaming** allows security teams, AI engineers, and compliance officers to:

- **Identify vulnerabilities** - Discover potential attack vectors and safety issues in AI systems
- **Test robustness** - Evaluate how AI models respond to adversarial inputs and edge cases
- **Ensure compliance** - Meet regulatory requirements for AI safety and responsible deployment
- **Prevent harm** - Catch potentially dangerous or biased outputs before they impact users
- **Build trust** - Demonstrate due diligence in AI safety to stakeholders and customers
- **Continuous monitoring** - Ongoing assessment of AI systems as they evolve

<Tip title="Confident AI's evals are 100% powered by DeepTeam">
  Our Red Teaming capabilities are based on industry-standard security frameworks
  and best practices from cybersecurity, adapted specifically for AI systems.

<Frame background="subtle" caption="⭐ DeepTeam Star Growth ⭐">
  [![Star History
  Chart](https://api.star-history.com/svg?repos=confident-ai/deepteam&type=Date)](https://www.star-history.com/#confident-ai/deepteam&Date)
</Frame>

We incorporate methodologies from NIST AI Risk Management Framework, OWASP Top 10 for LLMs, and MITRE ATLAS to provide comprehensive coverage of AI-specific threats.

</Tip>

## How AI Red Teaming works

Red Teaming on Confident AI has three core components:

<CardGroup cols={3}>
  <Card title="Risk Profiling" icon="triangle-exclamation">
    Automated assessment of your AI system's attack surface and vulnerability
    landscape.
  </Card>
  <Card title="Adversarial Testing" icon="crosshairs">
    Systematic probing with malicious prompts, jailbreaks, and edge cases.
  </Card>
  <Card title="Safety Monitoring" icon="shield-check">
    Continuous evaluation of AI outputs for harmful, biased, or inappropriate
    content.
  </Card>
</CardGroup>

You can run red teaming exercises on any AI application - from chatbots and RAG systems to complex agentic workflows and fine-tuned models.

<Note>
  Safety testing integrates seamlessly with your existing [LLM
  evaluation](/docs/llm-evaluation/quickstart) and
  [tracing](/docs/llm-tracing/introduction) workflows on Confident AI.
</Note>

## Key capabilities

- Comprehensive vulnerability scanning for prompt injection, data poisoning, and model extraction
- Automated jailbreak testing with 50+ attack patterns and techniques
- Bias and fairness assessment across protected characteristics
- Content safety evaluation for harmful, toxic, or inappropriate outputs
- Custom attack scenario creation for industry-specific threats
- Integration with existing CI/CD pipelines for continuous security testing
- Detailed risk reports with remediation recommendations
- Compliance reporting for regulatory frameworks (EU AI Act, NIST, etc.)

## Choose your quickstart

<CardGroup cols={2}>
  <Card
    title="Risk Profile Assessment"
    icon="chart-line"
    iconType="solid"
    href="/docs/red-teaming/risk-profile"
  >
    <div className='absolute top-4 right-4'>
      <Icon icon="arrow-up-right-from-square" />
    </div>

    **Best for:** Teams new to AI security who want to understand their risk landscape
    <br />

    - Automated vulnerability discovery
    - Risk scoring and prioritization
    - Attack surface mapping
    - Baseline security assessment

    *Get a comprehensive overview of your AI system's security posture*

  </Card>
  <Card
    title="Adversarial Testing"
    icon="crosshairs"
    iconType="solid"
    href="/docs/red-teaming/quickstart"
  >

    **Best for:** Security teams ready to conduct active penetration testing
    <br />

    <div className='absolute top-4 right-4'>
      <Icon icon="arrow-up-right-from-square" />
    </div>
    - Jailbreak and prompt injection testing
    - Custom attack scenario development
    - Continuous adversarial monitoring
    - Integration with existing security workflows

    *Actively probe your AI systems for vulnerabilities and weaknesses*

  </Card>
</CardGroup>

## FAQs

<AccordionGroup>
  <Accordion title="How is AI Red Teaming different from traditional security testing?">
    AI Red Teaming focuses on unique vulnerabilities specific to AI systems, such as prompt injection, model poisoning, and adversarial examples. Traditional security testing covers infrastructure and application security, while AI Red Teaming addresses the novel attack vectors introduced by machine learning models and natural language interfaces.

    [Learn more about AI-specific threats](/docs/red-teaming/framework-policies) in our frameworks guide.

  </Accordion>

  <Accordion title="What types of AI systems can be red teamed?">
    Our platform supports red teaming for all types of AI applications, including:
    - Conversational AI and chatbots
    - RAG (Retrieval-Augmented Generation) systems  
    - Multi-agent workflows and autonomous systems
    - Fine-tuned and custom models
    - AI-powered APIs and services

    Each system type has tailored attack scenarios and evaluation criteria.

  </Accordion>

  <Accordion title="Do I need security expertise to use Red Teaming?">
    No! Our platform is designed to be accessible to teams with varying security expertise. We provide:
    - Automated risk assessments with guided recommendations
    - Pre-built attack scenarios and test cases
    - Clear explanations of vulnerabilities and their impact
    - Step-by-step remediation guidance

    However, having security expertise on your team will help you get the most value from advanced features.

  </Accordion>

  <Accordion title="Can I run Red Teaming in production environments?">
    Yes, our Red Teaming platform supports both pre-production testing and continuous production monitoring. For production environments, we offer:
    - Non-disruptive monitoring modes
    - Configurable alert thresholds
    - Integration with existing monitoring systems
    - Safe testing protocols that don't impact user experience

    You can start with development environments and gradually expand to production as your confidence grows.

  </Accordion>

  <Accordion title="What is the pricing for Red Teaming features?">
    Red Teaming capabilities are enterprise-only. Contact our team at support@confident-ai.com for a free-trial today.

  </Accordion>
</AccordionGroup>
