---
slug: integrations/third-party/pydantic-ai
subtitle: Use Confident AI for LLM observability and evals for PydanticAI
---

## Overview

[Pydantic AI](https://ai.pydantic.dev/) is a python-native LLM agent framework built on the foundations of pydantic validation. Confident AI allows you to trace and evaluate Pydantic AI agents in just a few lines of code.

## Tracing Quickstart

<Steps>

<Step title="Install Dependencies">

Run the following command to install the required packages:

```bash
pip install pydantic-ai sdk -U deepeval
```

</Step>

<Step title="Configure Pydantic AI">

Instrument Pydantic AI with your Confident AI API key using `instrument_pydantic_ai`.

<Tabs>

<Tab title="Synchronous">

```python main.py {4} maxLines=100
from pydantic_ai import Agent
from deepeval.integrations.pydantic_ai import instrument_pydantic_ai

instrument_pydantic_ai(api_key="<your-confident-api-key>")

agent = Agent(
    "openai:gpt-4o-mini",
    system_prompt="Be concise, reply with one sentence.",
)
result = agent.run_sync("What are the LLMs?")
```

</Tab>

<Tab title="Asynchronous">

```python main.py {5} maxLines=100
import asyncio
from pydantic_ai import Agent
from deepeval.integrations.pydantic_ai import instrument_pydantic_ai

instrument_pydantic_ai(api_key="<your-confident-api-key>")

agent = Agent(
    "openai:gpt-4o-mini",
    system_prompt="Be concise, reply with one sentence.",
)

async def execute_agent():
    return await agent.run("What are the LLMs?")

result = asyncio.run(execute_agent())
```

</Tab>

</Tabs>

</Step>

<Step title="Run Pydantic AI">

Invoke your agent by executing the script:

```bash
python main.py
```

You can directly view the traces on [Confident AI](https://app.confident.ai) by clicking on the link in the output printed in the console.

</Step>

</Steps>

## Advanced Usage

### Logging prompts

If you are [managing prompts](/docs/llm-evaluation/prompt-optimization/prompt-versioning) on Confident AI and wish to log them, pass your `Prompt` object to the `Agent`.

```python main.py focus={2, 7-8, 13}
from pydantic_ai import Agent
from deepeval.prompt import Prompt
from deepeval.integrations.pydantic_ai import instrument_pydantic_ai

instrument_pydantic_ai()

prompt = Prompt(alias="asd")
prompt.pull(version="00.00.01")

agent = Agent(
    "openai:gpt-4o-mini",
    system_prompt="Be concise, reply with one sentence.",
    llm_prompt=prompt,
)

result = agent.run_sync("What are the LLMs?")
```

<Note>
  Logging prompts lets you attribute specific prompts to OpenAI Agent LLM spans.
  Be sure to **pull the prompt** before logging it, otherwise the prompt will
  not be visible on Confident AI.
</Note>

### Logging Threads

Threads are used to group related traces together, and are useful for chat apps, agents, or any multi-turn interactions. You can learn more about [threads](/docs/llm-tracing/advanced-features/threads) here. Pass the `thread_id` to the `run_sync` or `run` method.

```python main.py focus={13}
from pydantic_ai import Agent
from deepeval.integrations.pydantic_ai import instrument_pydantic_ai

instrument_pydantic_ai(api_key="<your-confident-api-key>")

agent = Agent(
    "openai:gpt-4o-mini",
    system_prompt="Be concise, reply with one sentence."
)

result = agent.run_sync(
    "What are the LLMs?",
    thread_id="test_thread_id_1",
)
```

### Trace attributes

Other trace attributes can also be passed to the `run_sync` or `run` method.

```python main.py maxLines=100 focus={12-16}
from pydantic_ai import Agent

instrument_pydantic_ai(api_key="<your-confident-api-key>")

agent = Agent(
    "openai:gpt-4o-mini",
    system_prompt="Be concise, reply with one sentence.",
)

result = agent.run_sync(
    "What are the LLMs?",
    name="Name of Trace",
    tags=["Tag 1", "Tag 2"],
    metadata={"Key": "Value"},
    thread_id="thread_21",
    user_id="user_21",
)
```

<Accordion title='View Trace Attributes'>
<ParamField path="name" type="str" required={false}>
  The name of the trace. [Learn more](/docs/llm-tracing/advanced-features/name).
</ParamField>

<ParamField path="tags" type="List[str]" required={false}>
  Tags are string labels that help you group related traces. [Learn
  more](/docs/llm-tracing/advanced-features/tags).
</ParamField>

<ParamField path="metadata" type="Dict" required={false}>
  Attach any metadata to the trace. [Learn
  more](/docs/llm-tracing/advanced-features/metadata).
</ParamField>

<ParamField path="thread_id" type="str" required={false}>
  Supply the thread or conversation ID to view and evaluate conversations.
  [Learn more](/docs/llm-tracing/advanced-features/threads).
</ParamField>

<ParamField path="user_id" type="str" required={false}>
  Supply the user ID to enable user analytics. [Learn
  more](/docs/llm-tracing/advanced-features/users).
</ParamField>

<Info>
  Each attribute is **optional**, and works the same way as the [native tracing
  features](/docs/llm-tracing/introduction) on Confident AI.
</Info>
</Accordion>

### Streaming Responses

You can also trace streamed responses with Pydantic AI.

```python main.py
import asyncio
from pydantic_ai import Agent
from deepeval.integrations.pydantic_ai import instrument_pydantic_ai

instrument_pydantic_ai(api_key="<your-confident-api-key>")

agent = Agent("openai:gpt-4o-mini", system_prompt="Be concise, reply with one sentence.")

async def main():
    user_prompt = 'What will the weather be like in Paris on Tuesday?'
    async with agent.run_stream(user_prompt) as run:
        async for output in run.stream_text():
            print(f'[Output] {output}')

asyncio.run(main())
```

### Verbose Tracing

Since Confident AI has an Opentelemetry compatible endpoint, you can use Pydantic AI's built-in ability to log traces in more detail by specifying `otlp=True`.

<Warning>
  Evals for Pydantic AI are **NOT supported** when using verbose tracing.
</Warning>

```python main.py focus={4-7} maxLines=100
from pydantic_ai import Agent
from deepeval.integrations.pydantic_ai import instrument_pydantic_ai

instrument_pydantic_ai(
    api_key="<your-confident-api-key>",
    otlp=True,
)

agent = Agent("openai:gpt-4o-mini", system_prompt="Be concise, reply with one sentence.")
result = agent.run_sync("What are the LLMs?")
```

## Evals Usage

### Online evals

You can run [online evals](/docs/llm-tracing/evaluations) on your Pydantic agent, which will run evaluations on all incoming traces on Confident AIâ€™s servers. This approach is recommended if your agent is in production.

<Steps>

<Step title="Create metric collection">

Create a metric collection on [Confident AI](https://app.confident.ai) with the metrics you wish to use to evaluate your Pydantic agent.

<Frame caption="Create metric collection">
  <video
    src="https://confident-docs.s3.us-east-1.amazonaws.com/integrations%3Athird-party-integrations%3Acreate-metric-collection-4k.mp4"
    controls
    autoPlay
  />
</Frame>

<Warning>
  Your metric collection must only contain metrics that only evaluate the input
  and actual output of your Pydantic agent.
</Warning>

</Step>

<Step title="Run evals">

You can run evals at both the trace and span level. We recommend creating separate [metric collections](/docs/metrics/metric-collections) for each component, since each requires its own evaluation criteria and metrics.

After instrumenting your Pydantic AI pass the metric collection name to the respective componens:

<Tabs>

<Tab title="Trace">
Pass the `metric_collection` parameter to the `run_sync` (or `run` for async mode) method.

```python main.py focus={9} maxLines=100
from pydantic_ai import Agent
from deepeval.integrations.pydantic_ai import instrument_pydantic_ai
instrument_pydantic_ai()

agent = Agent("openai:gpt-4o-mini", system_prompt="Be concise, reply with one sentence.")

result = agent.run_sync(
    "What are the LLMs?",
    metric_collection="test_collection_1"
)
```

</Tab>

<Tab title="Agent Span">

Pass the `agent_metric_collection` parameter to the `Agent` constructor.

```python main.py focus={7} maxLines=100
from pydantic_ai import Agent
from deepeval.integrations.pydantic_ai import instrument_pydantic_ai
instrument_pydantic_ai()

agent = Agent("openai:gpt-4o-mini",
    system_prompt="Be concise, reply with one sentence.",
    agent_metric_collection="test_collection_1"
)
result = agent.run_sync("What are the LLMs?")
```

</Tab>

<Tab title="LLM Span">

Pass the `llm_metric_collection` parameter to the `Agent` constructor.

```python main.py focus={7} maxLines=100
from pydantic_ai import Agent
from deepeval.integrations.pydantic_ai import instrument_pydantic_ai
instrument_pydantic_ai()

agent = Agent("openai:gpt-4o-mini",
    system_prompt="Be concise, reply with one sentence.",
    llm_metric_collection="test_collection_1"
)
result = agent.run_sync("What are the LLMs?")
```

</Tab>

<Tab title="Tool Span">

Use the `observe` decorator to pass the `metric_collection` parameter to the tool span.

```python main.py focus={2, 7-11, 14} maxLines=100
from pydantic_ai import Agent
from deepeval.tracing import observe
from deepeval.integrations.pydantic_ai import instrument_pydantic_ai

instrument_pydantic_ai()

@observe(type="tool", metric_collection="test_collection_1")
def get_weather(city: str) -> str:
    """Gets the weather for a given city."""
    return f"I don't know the weather for {city}."

agent = Agent(
    "openai:gpt-4o-mini",
    tools=[get_weather],
    system_prompt="You are a helpful weather agent.",
)
result = agent.run_sync("What is the weather in London?",)
```

</Tab>

</Tabs>

</Step>
<Success>
  All incoming traces will now be evaluated using metrics from your metric
  collection.
</Success>
</Steps>

### End-to-end evals

Running [end-to-end evals](/docs/llm-evaluation/single-turn/end-to-end) on your Pydantic agent evaluates your agent locally, and is the recommended approach if your agent is in a development or testing environment.

<Steps>

<Step title="Create metric">

```python
from deepeval.metrics import AnswerRelevancyMetric

answer_relevancy = AnswerRelevancyMetric(
    threshold=0.7,
    model="gpt-4o-mini",
    include_reason=True
)
```

</Step>

<Warning>
  Similar to online evals, you can only run end-to-end evals on metrics that
  evaluate the input and actual output of your Pydantic agent.
</Warning>

<Step title="Run evals">

Provide your metrics. Then, use the dataset's `evals_iterator` to invoke your Pydantic agent for each golden.

<Tabs>
<Tab title="Asynchronous">

```python main.py maxLines=100
import asyncio
from pydantic_ai import Agent
from deepeval.metrics import AnswerRelevancyMetric
from deepeval.integrations.pydantic_ai import instrument_pydantic_ai
from deepeval.dataset import EvaluationDataset, Golden

instrument_pydantic_ai()

agent = Agent("openai:gpt-4o-mini", system_prompt="Be concise, reply with one sentence.")

answer_relavancy_metric = AnswerRelevancyMetric()
dataset = EvaluationDataset(goldens=[Golden(input="What's 7 * 8?"), Golden(input="What's 7 * 6?")])

for golden in dataset.evals_iterator():
    task = asyncio.create_task(agent.run(
        golden.input,
        metrics=[answer_relavancy_metric],
    ))
    dataset.evaluate(task)
```

</Tab>
</Tabs>

<Success>
  This will automatically generate a test run with evaluated traces using inputs
  from your dataset.
</Success>

</Step>
</Steps>

### View on Confident AI

You can view the evals on [Confident AI](https://app.confident.ai) by clicking on the link in the output printed in the console.

<Frame>
  <video
    src="https://confident-bucket.s3.us-east-1.amazonaws.com/end-to-end%3Apydantic-1080.mp4"
    controls
    autoPlay
    playsInline
  />
</Frame>
````
