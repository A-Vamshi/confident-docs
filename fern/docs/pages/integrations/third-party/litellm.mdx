---
slug: integrations/third-party/litellm
description: Get started with Confident AI for LLM evaluation and observability
---

[LiteLLM](https://www.litellm.ai/) is a tool that makes it easy for developers to connect to and use many different large language models (LLMs)—like those from OpenAI, Anthropic, Hugging Face, and more—through a single, simple interface.

## Tracing Quickstart

Confident AI integrates with LiteLLM to trace your LLM calls for both **Proxy** and **SDK** in the **Observatory**.

<Note>
  LiteLLM **[Proxy](https://docs.litellm.ai/docs/simple_proxy)** is a server (or
  gateway) that acts as a central point for accessing many different language
  models through a single API endpoint whereas the
  **[SDK](https://docs.litellm.ai/docs/#basic-usage)** is a client library that
  allows you to use LiteLLM in your own application.
</Note>

### Setup via SDK

Install the LiteLLM SDK:

<Steps>
<Step title="Install Dependencies">

```bash
pip install litellm
```

</Step>

<Step title="Configure LiteLLM">

Use `litellm.success_callback` and `litellm.failure_callback` to trace your LLM calls:

```python showLineNumbers {8-9}
import os
import time
import litellm

os.environ['OPENAI_API_KEY']='<your-openai-api-key>'
os.environ['CONFIDENT_API_KEY']='<your-confident-api-key>'

litellm.success_callback = ["deepeval"]
litellm.failure_callback = ["deepeval"]

response = litellm.completion(
    model="gpt-3.5-turbo",
    messages=[
        {"role": "user", "content": "What's the weather like in San Francisco?"}
    ],
)
```

</Step>
</Steps>

### Setup via Proxy

<Steps>
<Step title="Configure LiteLLM">
Add **deepeval** to the `success_callback` and `failure_callback` in the `config.yaml` file.

```yaml config.yaml {6-7}
model_list:
  - model_name: gpt-4o
    litellm_params:
      model: gpt-4o
litellm_settings:
  success_callback: ["deepeval"]
  failure_callback: ["deepeval"]
```

</Step>

<Step title="Set environment variables">
Add your OpenAI and Confident API keys in `.env` file.

```bash
OPENAI_API_KEY=<your-openai-api-key>
CONFIDENT_API_KEY=<your-confident-api-key>
```

</Step>

<Step title="Start server">
Run the following command to start the proxy server:

```bash
litellm --config config.yaml --debug
```

</Step>

<Step title="Make a request">

Run the following command to make a request to the proxy server:

```bash
curl -X POST 'http://0.0.0.0:4000/chat/completions' \
-H 'Content-Type: application/json' \
-H 'Authorization: Bearer sk-1234' \
-d '{
    "model": "gpt-3.5-turbo",
    "messages": [
      {
        "role": "system",
        "content": "You are a helpful math tutor. Guide the user through the solution step by step."
      },
      {
        "role": "user",
        "content": "how can I solve 8x + 7 = -23"
      }
    ]
}'
```

</Step>
</Steps>

<Note>
  There are two ways to start the LiteLLM proxy server from CLI i.e., either
  using **pip package** or **docker container**. We have used the **pip
  package** in the example above. Refer to the [LiteLLM
  documentation](https://docs.litellm.ai/docs/#quick-start-proxy---cli) for more
  information.
</Note>
