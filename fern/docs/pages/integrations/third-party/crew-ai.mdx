---
slug: integrations/third-party/crew-ai
description: Get started with Confident AI for LLM evaluation and observability
---

[CrewAI](https://www.crew.ai) is a lean, lightning-fast Python framework for creating autonomous AI agents tailored to any scenario. Confident AI allows you to trace and evaluate CrewAI workflows with just a single line of code.

## Quickstart

<Steps>

<Step title="Install Dependencies">

Run the following command to install the required packages:

```bash
pip install -U deepeval crewai
```

</Step>

<Step title="Configure CrewAI">

Instrument CrewAI with your Confident AI API key using `instrument_crewai`.

```python main.py {6}
import os
import time
from crewai import Task, Crew, Agent
from deepeval.integrations.crewai import instrument_crewai

instrument_crewai(api_key="<your-confident-api-key>")
os.environ["OPENAI_API_KEY"] = "<your-openai-api-key>"

# Define your agents with roles and goals
coder = Agent(
    role='Consultant',
    goal='Write clear, concise explanation.',
    backstory='An expert consultant with a keen eye for software trends.',
)

# Create tasks for your agents
task1 = Task(description="Explain the given topic", expected_output="A clear and concise explanation.", agent=coder)

# Instantiate and kickoff your crew
crew = Crew(agents=[coder], tasks=[task1])
result = crew.kickoff({"input": "What are the LLMs?"})
```

</Step>

<Step title="Run CrewAI">

Kickoff your crew by executing the script:

```bash
python main.py
```

</Step>

You can directly view the traces on [Confident AI](https://app.confident.ai) by clicking on the link in the output printed in the console.

</Steps>

## Advanced Usage

### Online evals

To run [online evals](https://documentation.confident-ai.com/docs/llm-tracing/evaluations) on your CrewAI agent, replace your CrewAI `Agent` with DeepEvalâ€™s, and provide a metric collection name as an argument to the `Agent`.

```python main.py {1, 8}
from deepeval.integrations.crewai.agent import Agent

# Define your agents with metric collection name
coder = Agent(
    role='Consultant',
    goal='Write clear, concise explanation.',
    backstory='An expert consultant with a keen eye for software trends.',
    metric_collection="<your-metric-collection-name>"
)
...

# Kickoff your crew
result = crew.kickoff({"input": "What are the LLMs?"})
```

<ParamField path="metric_collection" type="str" required={false}>
  The name of the metric collection. To create a metric collection, visit
  [here](/docs/llm-evaluation/metrics/create-on-the-cloud).
</ParamField>

<Warning>
  Your metric collection cannot contain metrics that require the
  `retrieval_context`, `context`, or `expected_tools`.
</Warning>

### End-to-end evals

To run [end-to-end evals](https://documentation.confident-ai.com/docs/llm-evaluation/end-to-end-evals) on your CrewAI agent, supply `metrics` to DeepEval's `Agent` and use the dataset's `evals_iterator` to invoke your agent for each golden.

```python main.py {2, 11}
from deepeval.integrations.crewai.agent import Agent
from deepeval.metrics import AnswerRelevancyMetric
from deepeval.dataset import EvaluationDataset, Golden
...

# Create a metric
coder = Agent(
    role="Consultant",
    goal="Write clear, concise explanation.",
    backstory="An expert consultant with a keen eye for software trends.",
    metrics=[AnswerRelevancyMetric()],
)
...

# Create goldens
goldens = [
    Golden(input="What are Transformers in AI?"),
    Golden(input="What is the biggest open source database?"),
    Golden(input="What are LLMs?"),
]
dataset = EvaluationDataset(goldens=goldens)

# Run evaluation for each golden
for golden in dataset.evals_iterator():
    result = crew.kickoff(inputs={"input": golden.input})
```

<ParamField path="metrics" type="List[BaseMetric]" required={false}>
  The metrics to evaluate your CrewAI trace. Does not include metrics requiring
  `retrieval_context`, `context`, or `expected_tools`.
</ParamField>

<Note>
This will automatically create a generate a test run with evaluated traces.

</Note>
