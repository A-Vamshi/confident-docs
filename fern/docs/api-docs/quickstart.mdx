---
title: Quickstart
subtitle: 5 min quickstart guide for Confident AI's Evals API
slug: api-reference/quickstart
---

## Overview

Confident AI's Evals API allows you to run online evaluations on test cases, traces, spans, and threads. This 5 minute quickstart will teach you how to:

- Create **Metric Collections** to run your online evaluations.
- Use the `/v1/evaluate` endpoint to run your first eval.

## Run Your First Eval

Here's a step-by-step guide on how to run your first online evaluation using the Evals API.

<Steps>
  <Step title="Get Your API Key">
    Get your Project API Key from the Confident AI platform's project settings. Click here to [learn more](/docs/api-reference/authentication).
  </Step>

  <Step title="Create A Metric Collection">
    A metric collection is a list of metrics that are used to evaluate your test cases. It can either consist of single-turn or multi-turn metrics but not both. Learn more about [metric collections here](/docs/metrics/metric-collections).

    <Tabs>
        <Tab title="Create">
            You can create a metric collection using the POST [`/v1/metric-collections`](/docs/api-reference/metric-collections/create-metric-collection) endpoint. Note that all metric collections must have a unique name within your project.

            <EndpointRequestSnippet
                endpoint="POST /v1/metric-collections"
            />

        </Tab>
        <Tab title="List">
            You can see all the metric collections you have in your project using the GET [`/v1/metric-collections`](/docs/api-reference/metric-collections/list-metric-collections) endpoint.

            <EndpointRequestSnippet
                endpoint="GET /v1/metric-collections"
            />

        </Tab>
    </Tabs>
  </Step>
  
  <Step title="Run Evaluations">
    To run an evaluation, provide the name of the metric collection and the other relevant details based on your use case:

    <Tabs>
        <Tab title="Single Turn">

          Add a list of **llmTestCases** in your request body to run single-turn evaluations. Click here to see the data model of [llmTestCases](/docs/api-reference/evaluation/evaluate-llm#request.body.llmTestCases).
            
          <EndpointRequestSnippet
              endpoint="POST /v1/evaluate"
              example="Single-Turn"
          />

        </Tab>
        <Tab title="Multi Turn">

          Add a list of **conversationalTestCases** in your request body to run multi-turn evaluations. Click here to see the data model of [conversationalTestCases](/docs/api-reference/evaluation/evaluate-llm#request.body.conversationalTestCases).

          <EndpointRequestSnippet
              endpoint="POST /v1/evaluate"
              example="Multi-Turn"
          />

        </Tab>
        <Tab title="Span">
          Get the `uuid` of the span you want to evaluate from the Confident AI platform's Observatory. To learn more about this endpoint [click here](/docs/api-reference/evaluation/evaluate-span).
            
          <EndpointRequestSnippet
              endpoint="POST /v1/evaluate/spans/{uuid}"
              example="Single-Turn"
          />

        </Tab>
        <Tab title="Trace">
          Get the `uuid` of the trace you want to evaluate from the Confident AI platform's Observatory. To learn more about this endpoint [click here](/docs/api-reference/evaluation/evaluate-trace).
            
          <EndpointRequestSnippet
              endpoint="POST /v1/evaluate/traces/{uuid}"
              example="Single-Turn"
          />

        </Tab>
        <Tab title="Thread">
          Get the `id` of the trace you want to evaluate from the Confident AI platform's Observatory. To learn more about this endpoint [click here](/docs/api-reference/evaluation/evaluate-thread).
            
          <EndpointRequestSnippet
              endpoint="POST /v1/evaluate/threads/{id}"
              example="Single-Turn"
          />

        </Tab>
    </Tabs>

  </Step>
</Steps>

The `/v1/evaluate` API endpoint will create a test run on Confident AI and return the following response:

<EndpointResponseSnippet endpoint="POST /v1/evaluate" />

**ðŸŽ‰ Congratulations!** You just ran your first evaluation on Confident AI via the Evals API.

## View Results on Confident AI

After running an eval using our Evals API, your test results will be automatically stored on the Confident AI platform in a [comprehensive report format](/docs/llm-evaluation/dashboards/testing-reports). You can also separate the test results using the `TEST-RUN-ID` from the API response.

Here's an example report you'll get on the platform whenever you call the `/v1/evaluate` endpoint:

<Frame caption="Test Reports on Confident AI">
  <video
    src="https://confident-docs.s3.us-east-1.amazonaws.com/evaluation:single-turn-e2e-report.mp4"
    controls
    autoPlay
  />
</Frame>
