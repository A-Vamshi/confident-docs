---
title: Quickstart
subtitle: 5 min quickstart guide for Confident AI's Evals API
slug: api-reference/quickstart
---

## Overview

Confident AI's Evals API allows you to run online evaluations on test cases, traces, spans, and threads. This 5-minute quickstart will allow you to run your first evaluation by walking you through:

- Create a **metric collection**
- Use the `/v1/evaluate` endpoint to create a **test run**

## Run Your First Eval

Here's a step-by-step guide on how to run your first online evaluation using the Evals API.

<Steps>
  <Step title="Get your API key">
    Create a free account at [https://app.confident-ai.com](https://app.confident-ai.com), and get your **Project API Key**.

    <Tip>
        Make sure you're not copying your _organization API key_.
    </Tip>

  </Step>

  <Step title="Create a metric collection">

    You can create a metric collection containing the metric you wish to run evals with using the POST [`/v1/metric-collections`](/docs/api-reference/metric-collections/create-metric-collection) endpoint. Note that all metric collections must have a unique name within your project.

    <EndpointRequestSnippet
        endpoint="POST /v1/metric-collections"
    />

  </Step>
  
  <Step title="Create test run">
    To run an evaluation, provide the name of the metric collection a list of `"llmTestCases"` in your request body to run single-turn evaluations.

    <EndpointRequestSnippet
        endpoint="POST /v1/evaluate"
        example="Single-Turn"
    />

    **ðŸŽ‰ Congratulations!** You just successfully ran your first evaluation on Confident AI via the Evals API.

    <Note>
    The `/v1/evaluate` API endpoint will create a test run on Confident AI and return the following response:

    <EndpointResponseSnippet endpoint="POST /v1/evaluate" />

    </Note>

  </Step>
  <Step title="Verify test run on the UI">
  
After running an eval using our Evals API, your test results will be automatically stored on the Confident AI platform in a [comprehensive report format](/docs/llm-evaluation/dashboards/testing-reports). You can also separate the test results using the `TEST-RUN-ID` from the API response.

<Frame caption="Test Reports on Confident AI">
  <video
    src="https://confident-docs.s3.us-east-1.amazonaws.com/evaluation:single-turn-e2e-report.mp4"
    controls
    autoPlay
  />
</Frame>

  </Step>
</Steps>

## Next Steps

Now that you've run your first online evaluation, explore these next steps to go deeper with Confident AI:

- **Custom Datasets** â€” Create custom datasets using the [datasets endpoint](/docs/api-reference/datasets/push-dataset).
- **Prompt Templates** â€” Iterate and version your LLM prompts directly through the [prompts endpoint](/docs/api-reference/prompt/get-prompt).
- **Human Annotations** â€” Annotate your evaluations to enable human-in-the-loop feedback to guide metric tuning and reinforce quality with the [annotation endpoint](/docs/api-reference/annotations/create-annotation).
